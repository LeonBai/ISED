{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756b9023-fe3d-43f9-b069-108c779190fd",
   "metadata": {},
   "source": [
    "# LFADS IMPLEMENTATION \n",
    "## Most LFADS scripts comes from: https://github.com/lyprince/lfads_demo\n",
    "## Non-stitched and stitched mode \n",
    "\n",
    "non-stitch: treat entire time series as batch_size == 1; single trial, \n",
    "\n",
    "stitched: break time series into segments (subsequence); then treat them as mutiple trials, batch_size == data.shape[0]\n",
    "\n",
    "Output:Time x extracted factors \n",
    "\n",
    "Caution: since stitched mode returns Trial(T) x window x extracted factprs; we averaged the middle axis to get T x extracted factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0216cf52-c04c-436f-8c6d-dda218673db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb5d1-bc24-4403-817f-8d3066b0d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd lfads_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af705b36-5694-4b2e-a71a-d38d16fe40fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m np \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_np\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lfads-torch/lib/python3.9/site-packages/torch/__init__.py:2688\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_np'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'; print('Using device: %s'%device)\n",
    "\n",
    "import torchvision\n",
    "import os\n",
    "import yaml\n",
    "from lfads import LFADS_Net\n",
    "from utils import read_data, load_parameters, save_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de9eb055-cc34-4762-bd30-9dbed0a9523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../SeED/dataset/Xs.pkl','rb') as file:\n",
    "    \n",
    "    raw_data = pickle.load(file)\n",
    "    \n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "\n",
    "for i in range(3):\n",
    "    X_normalized = preprocessing.MinMaxScaler().fit_transform(raw_data[i])\n",
    "\n",
    "    train_data = X_normalized[:500]\n",
    "    test_data = X_normalized[500:500+500]\n",
    "    X_trains.append(train_data)\n",
    "    X_tests.append(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "460b8d67-df00-43df-b2f6-e3d258ec9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401, 100, 100)\n",
      "(401, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "## Stitched-mode\n",
    "\n",
    "\n",
    "L, S = 100, 1\n",
    "X_train_windows = []\n",
    "for start in range(0, X_trains[0].shape[0] - L + 1, S):\n",
    "    X_train_windows.append(X_trains[0][start:start+L])\n",
    "X_train_windows = np.stack(X_train_windows)  \n",
    "print (X_train_windows.shape)\n",
    "\n",
    "\n",
    "X_test_windows = []\n",
    "for start in range(0, X_tests[0].shape[0] - L + 1, S):\n",
    "    X_test_windows.append(X_tests[0][start:start+L])\n",
    "X_test_windows = np.stack(X_test_windows)  \n",
    "print (X_test_windows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8fc457f-772b-465f-b839-391eb329901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([401, 100, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Validation data == train_data here \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Train_data = torch.Tensor(np.expand_dims(X_trains[0],axis = 0)).to(device)\n",
    "#Test_data = torch.Tensor(np.expand_dims(X_tests[0],axis = 0)).to(device)\n",
    "\n",
    "Train_data = torch.Tensor(X_train_windows).to(device)\n",
    "Test_data = torch.Tensor(X_test_windows).to(device)\n",
    "Valid_data = Train_data\n",
    "Valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b04f1dc-a5dd-4806-8c5a-d54bce8bcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds      = torch.utils.data.TensorDataset(Train_data)\n",
    "valid_ds      = torch.utils.data.TensorDataset(Valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3257fbe9-6dab-4ba7-8394-9e20babdd7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbi/wjbai/lfads-torch/lfads_demo/utils.py:91: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return yaml.load(open(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'chaotic_rnn',\n",
       " 'run_name': 'demo',\n",
       " 'g_dim': 100,\n",
       " 'u_dim': 1,\n",
       " 'factors_dim': 10,\n",
       " 'g0_encoder_dim': 100,\n",
       " 'c_encoder_dim': 100,\n",
       " 'controller_dim': 100,\n",
       " 'g0_prior_kappa': 0.1,\n",
       " 'u_prior_kappa': 0.1,\n",
       " 'keep_prob': 0.95,\n",
       " 'clip_val': 5.0,\n",
       " 'max_norm': 200,\n",
       " 'learning_rate': 0.01,\n",
       " 'learning_rate_min': 1e-05,\n",
       " 'learning_rate_decay': 0.95,\n",
       " 'scheduler_on': True,\n",
       " 'scheduler_patience': 6,\n",
       " 'scheduler_cooldown': 6,\n",
       " 'kl_weight_schedule_start': 0,\n",
       " 'kl_weight_schedule_dur': 2000,\n",
       " 'l2_weight_schedule_start': 0,\n",
       " 'l2_weight_schedule_dur': 2000,\n",
       " 'epsilon': 0.1,\n",
       " 'betas': (0.9, 0.99),\n",
       " 'l2_gen_scale': 2000,\n",
       " 'l2_con_scale': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = load_parameters('parameters_sim.yaml')\n",
    "save_parameters(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78844efd-fd10-4ed8-aa8f-e8a74696b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 5787\n"
     ]
    }
   ],
   "source": [
    "model = LFADS_Net(inputs_dim = 100, T = 100, dt = 1, device=device,\n",
    "                 model_hyperparams=hyperparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d216b872-e7e8-41b4-9cc4-6e622e5000ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch:    1, Step:     9, training loss: 8986.411, validation loss: 8676.522\n",
      "Epoch:    2, Step:    18, training loss: 7825.355, validation loss: 7868.275\n",
      "Epoch:    3, Step:    27, training loss: 7478.233, validation loss: 7639.073\n",
      "Epoch:    4, Step:    36, training loss: 7425.668, validation loss: 7599.618\n",
      "Epoch:    5, Step:    45, training loss: 7408.300, validation loss: 7569.018\n",
      "Epoch:    6, Step:    54, training loss: 7405.428, validation loss: 7503.032\n",
      "Epoch:    7, Step:    63, training loss: 7404.062, validation loss: 7463.556\n",
      "Epoch:    8, Step:    72, training loss: 7403.507, validation loss: 7449.185\n",
      "Epoch:    9, Step:    81, training loss: 7399.266, validation loss: 7423.125\n",
      "Epoch:   10, Step:    90, training loss: 7402.733, validation loss: 7417.221\n",
      "Epoch:   11, Step:    99, training loss: 7400.933, validation loss: 7407.453\n",
      "Epoch:   12, Step:   108, training loss: 7397.811, validation loss: 7405.528\n",
      "Epoch:   13, Step:   117, training loss: 7397.051, validation loss: 7403.290\n",
      "Epoch:   14, Step:   126, training loss: 7397.393, validation loss: 7402.427\n",
      "Epoch:   15, Step:   135, training loss: 7400.381, validation loss: 7401.504\n",
      "Epoch:   16, Step:   144, training loss: 7395.577, validation loss: 7401.588\n",
      "Epoch:   17, Step:   153, training loss: 7398.291, validation loss: 7401.613\n",
      "Epoch:   18, Step:   162, training loss: 7396.753, validation loss: 7401.699\n",
      "Epoch:   19, Step:   171, training loss: 7395.695, validation loss: 7399.764\n",
      "Epoch:   20, Step:   180, training loss: 7395.459, validation loss: 7400.874\n",
      "Epoch:   21, Step:   189, training loss: 7398.317, validation loss: 7399.698\n",
      "Epoch:   22, Step:   198, training loss: 7399.803, validation loss: 7399.893\n",
      "Learning rate decreased to 0.00950000\n",
      "Epoch:   23, Step:   207, training loss: 7396.760, validation loss: 7399.960\n",
      "Epoch:   24, Step:   216, training loss: 7395.807, validation loss: 7399.881\n",
      "Epoch:   25, Step:   225, training loss: 7400.260, validation loss: 7399.284\n",
      "Epoch:   26, Step:   234, training loss: 7399.967, validation loss: 7400.744\n",
      "Epoch:   27, Step:   243, training loss: 7399.751, validation loss: 7398.938\n",
      "Epoch:   28, Step:   252, training loss: 7397.427, validation loss: 7398.730\n",
      "Epoch:   29, Step:   261, training loss: 7395.187, validation loss: 7398.686\n",
      "Epoch:   30, Step:   270, training loss: 7396.569, validation loss: 7398.594\n",
      "Epoch:   31, Step:   279, training loss: 7396.216, validation loss: 7398.540\n",
      "Epoch:   32, Step:   288, training loss: 7399.804, validation loss: 7398.635\n",
      "Epoch:   33, Step:   297, training loss: 7396.843, validation loss: 7398.938\n",
      "Epoch:   34, Step:   306, training loss: 7398.783, validation loss: 7398.602\n",
      "Epoch:   35, Step:   315, training loss: 7397.934, validation loss: 7399.954\n",
      "Epoch:   36, Step:   324, training loss: 7395.966, validation loss: 7399.259\n",
      "Epoch:   37, Step:   333, training loss: 7398.657, validation loss: 7399.491\n",
      "Epoch:   38, Step:   342, training loss: 7396.209, validation loss: 7398.649\n",
      "Epoch:   39, Step:   351, training loss: 7396.716, validation loss: 7398.306\n",
      "Epoch:   40, Step:   360, training loss: 7394.805, validation loss: 7398.793\n",
      "Epoch:   41, Step:   369, training loss: 7398.020, validation loss: 7398.767\n",
      "Epoch:   42, Step:   378, training loss: 7395.965, validation loss: 7399.836\n",
      "Epoch:   43, Step:   387, training loss: 7395.612, validation loss: 7399.030\n",
      "Epoch:   44, Step:   396, training loss: 7400.901, validation loss: 7398.220\n",
      "Learning rate decreased to 0.00902500\n",
      "Epoch:   45, Step:   405, training loss: 7398.915, validation loss: 7399.878\n",
      "Epoch:   46, Step:   414, training loss: 7400.455, validation loss: 7398.369\n",
      "Epoch:   47, Step:   423, training loss: 7399.363, validation loss: 7399.066\n",
      "Epoch:   48, Step:   432, training loss: 7395.518, validation loss: 7398.163\n",
      "Epoch:   49, Step:   441, training loss: 7400.955, validation loss: 7398.527\n",
      "Epoch:   50, Step:   450, training loss: 7397.920, validation loss: 7399.038\n",
      "Epoch:   51, Step:   459, training loss: 7399.071, validation loss: 7398.051\n",
      "Epoch:   52, Step:   468, training loss: 7397.020, validation loss: 7398.493\n",
      "Epoch:   53, Step:   477, training loss: 7400.838, validation loss: 7397.971\n",
      "Epoch:   54, Step:   486, training loss: 7398.037, validation loss: 7398.471\n",
      "Epoch:   55, Step:   495, training loss: 7394.292, validation loss: 7399.230\n",
      "Epoch:   56, Step:   504, training loss: 7400.202, validation loss: 7398.255\n",
      "Epoch:   57, Step:   513, training loss: 7398.233, validation loss: 7399.109\n",
      "Epoch:   58, Step:   522, training loss: 7399.430, validation loss: 7398.002\n",
      "Epoch:   59, Step:   531, training loss: 7395.845, validation loss: 7398.757\n",
      "Epoch:   60, Step:   540, training loss: 7399.754, validation loss: 7398.528\n",
      "Epoch:   61, Step:   549, training loss: 7397.846, validation loss: 7398.153\n",
      "Epoch:   62, Step:   558, training loss: 7394.462, validation loss: 7397.827\n",
      "Epoch:   63, Step:   567, training loss: 7400.956, validation loss: 7397.796\n",
      "Learning rate decreased to 0.00857375\n",
      "Epoch:   64, Step:   576, training loss: 7396.491, validation loss: 7399.560\n",
      "Epoch:   65, Step:   585, training loss: 7397.341, validation loss: 7398.255\n",
      "Epoch:   66, Step:   594, training loss: 7397.077, validation loss: 7397.939\n",
      "Epoch:   67, Step:   603, training loss: 7397.074, validation loss: 7398.424\n",
      "Epoch:   68, Step:   612, training loss: 7397.153, validation loss: 7397.999\n",
      "Epoch:   69, Step:   621, training loss: 7400.125, validation loss: 7398.245\n",
      "Epoch:   70, Step:   630, training loss: 7399.268, validation loss: 7399.559\n",
      "Epoch:   71, Step:   639, training loss: 7398.069, validation loss: 7398.182\n",
      "Epoch:   72, Step:   648, training loss: 7396.123, validation loss: 7399.112\n",
      "Epoch:   73, Step:   657, training loss: 7394.814, validation loss: 7397.991\n",
      "Epoch:   74, Step:   666, training loss: 7400.549, validation loss: 7397.708\n",
      "Learning rate decreased to 0.00814506\n",
      "Epoch:   75, Step:   675, training loss: 7397.111, validation loss: 7398.488\n",
      "Epoch:   76, Step:   684, training loss: 7396.384, validation loss: 7397.852\n",
      "Epoch:   77, Step:   693, training loss: 7399.990, validation loss: 7398.451\n",
      "Epoch:   78, Step:   702, training loss: 7398.876, validation loss: 7398.847\n",
      "Epoch:   79, Step:   711, training loss: 7399.795, validation loss: 7397.860\n",
      "Epoch:   80, Step:   720, training loss: 7399.281, validation loss: 7398.444\n",
      "Epoch:   81, Step:   729, training loss: 7398.157, validation loss: 7398.838\n",
      "Epoch:   82, Step:   738, training loss: 7399.594, validation loss: 7397.905\n",
      "Epoch:   83, Step:   747, training loss: 7394.659, validation loss: 7397.569\n",
      "Epoch:   84, Step:   756, training loss: 7397.428, validation loss: 7397.593\n",
      "Epoch:   85, Step:   765, training loss: 7396.751, validation loss: 7399.212\n",
      "Epoch:   86, Step:   774, training loss: 7400.061, validation loss: 7398.022\n",
      "Learning rate decreased to 0.00773781\n",
      "Epoch:   87, Step:   783, training loss: 7399.636, validation loss: 7398.059\n",
      "Epoch:   88, Step:   792, training loss: 7398.577, validation loss: 7398.062\n",
      "Epoch:   89, Step:   801, training loss: 7396.166, validation loss: 7397.701\n",
      "Epoch:   90, Step:   810, training loss: 7399.927, validation loss: 7397.613\n",
      "Epoch:   91, Step:   819, training loss: 7395.646, validation loss: 7398.069\n",
      "Epoch:   92, Step:   828, training loss: 7395.795, validation loss: 7397.944\n",
      "Epoch:   93, Step:   837, training loss: 7395.200, validation loss: 7397.750\n",
      "Epoch:   94, Step:   846, training loss: 7397.038, validation loss: 7397.598\n",
      "Epoch:   95, Step:   855, training loss: 7398.233, validation loss: 7398.275\n",
      "Epoch:   96, Step:   864, training loss: 7394.365, validation loss: 7397.643\n",
      "Epoch:   97, Step:   873, training loss: 7398.801, validation loss: 7397.922\n",
      "Learning rate decreased to 0.00735092\n",
      "Epoch:   98, Step:   882, training loss: 7399.674, validation loss: 7398.976\n",
      "Epoch:   99, Step:   891, training loss: 7397.307, validation loss: 7397.659\n",
      "Epoch:  100, Step:   900, training loss: 7400.962, validation loss: 7397.668\n",
      "Epoch:  101, Step:   909, training loss: 7399.844, validation loss: 7398.877\n",
      "Epoch:  102, Step:   918, training loss: 7397.627, validation loss: 7397.896\n",
      "Epoch:  103, Step:   927, training loss: 7399.571, validation loss: 7397.431\n",
      "Epoch:  104, Step:   936, training loss: 7395.116, validation loss: 7398.013\n",
      "Epoch:  105, Step:   945, training loss: 7400.002, validation loss: 7397.458\n",
      "Epoch:  106, Step:   954, training loss: 7396.005, validation loss: 7398.618\n",
      "Epoch:  107, Step:   963, training loss: 7398.791, validation loss: 7398.253\n",
      "Epoch:  108, Step:   972, training loss: 7399.471, validation loss: 7397.955\n",
      "Epoch:  109, Step:   981, training loss: 7399.355, validation loss: 7397.765\n",
      "Epoch:  110, Step:   990, training loss: 7396.499, validation loss: 7398.096\n",
      "Epoch:  111, Step:   999, training loss: 7396.471, validation loss: 7397.869\n",
      "Epoch:  112, Step:  1008, training loss: 7400.423, validation loss: 7397.607\n",
      "Learning rate decreased to 0.00698337\n",
      "Epoch:  113, Step:  1017, training loss: 7399.702, validation loss: 7399.323\n",
      "Epoch:  114, Step:  1026, training loss: 7398.292, validation loss: 7397.899\n",
      "Epoch:  115, Step:  1035, training loss: 7399.768, validation loss: 7397.654\n",
      "Epoch:  116, Step:  1044, training loss: 7397.829, validation loss: 7397.626\n",
      "Epoch:  117, Step:  1053, training loss: 7399.588, validation loss: 7397.562\n",
      "Epoch:  118, Step:  1062, training loss: 7398.645, validation loss: 7398.417\n",
      "Epoch:  119, Step:  1071, training loss: 7399.782, validation loss: 7397.853\n",
      "Learning rate decreased to 0.00663420\n",
      "Epoch:  120, Step:  1080, training loss: 7394.590, validation loss: 7397.452\n",
      "Epoch:  121, Step:  1089, training loss: 7400.502, validation loss: 7397.374\n",
      "Epoch:  122, Step:  1098, training loss: 7397.946, validation loss: 7399.081\n",
      "Epoch:  123, Step:  1107, training loss: 7398.825, validation loss: 7398.801\n",
      "Epoch:  124, Step:  1116, training loss: 7394.682, validation loss: 7397.471\n",
      "Epoch:  125, Step:  1125, training loss: 7400.718, validation loss: 7397.439\n",
      "Learning rate decreased to 0.00630249\n",
      "Epoch:  126, Step:  1134, training loss: 7397.049, validation loss: 7398.694\n",
      "Epoch:  127, Step:  1143, training loss: 7398.259, validation loss: 7398.237\n",
      "Epoch:  128, Step:  1152, training loss: 7399.140, validation loss: 7397.346\n",
      "Epoch:  129, Step:  1161, training loss: 7399.667, validation loss: 7398.069\n",
      "Epoch:  130, Step:  1170, training loss: 7398.905, validation loss: 7398.115\n",
      "Epoch:  131, Step:  1179, training loss: 7399.721, validation loss: 7397.676\n",
      "Epoch:  132, Step:  1188, training loss: 7395.071, validation loss: 7397.696\n",
      "Epoch:  133, Step:  1197, training loss: 7396.660, validation loss: 7397.479\n",
      "Epoch:  134, Step:  1206, training loss: 7396.159, validation loss: 7397.777\n",
      "Epoch:  135, Step:  1215, training loss: 7399.586, validation loss: 7397.430\n",
      "Epoch:  136, Step:  1224, training loss: 7399.123, validation loss: 7398.249\n",
      "Epoch:  137, Step:  1233, training loss: 7396.694, validation loss: 7397.554\n",
      "Epoch:  138, Step:  1242, training loss: 7399.882, validation loss: 7397.743\n",
      "Learning rate decreased to 0.00598737\n",
      "Epoch:  139, Step:  1251, training loss: 7400.095, validation loss: 7398.439\n",
      "Epoch:  140, Step:  1260, training loss: 7396.328, validation loss: 7397.827\n",
      "Epoch:  141, Step:  1269, training loss: 7398.022, validation loss: 7397.499\n",
      "Epoch:  142, Step:  1278, training loss: 7396.185, validation loss: 7397.396\n",
      "Epoch:  143, Step:  1287, training loss: 7399.143, validation loss: 7397.580\n",
      "Epoch:  144, Step:  1296, training loss: 7399.346, validation loss: 7398.280\n",
      "Epoch:  145, Step:  1305, training loss: 7396.747, validation loss: 7397.881\n",
      "Epoch:  146, Step:  1314, training loss: 7396.428, validation loss: 7397.631\n",
      "Epoch:  147, Step:  1323, training loss: 7395.997, validation loss: 7397.528\n",
      "Epoch:  148, Step:  1332, training loss: 7397.925, validation loss: 7397.371\n",
      "Epoch:  149, Step:  1341, training loss: 7399.650, validation loss: 7398.479\n",
      "Learning rate decreased to 0.00568800\n",
      "Epoch:  150, Step:  1350, training loss: 7399.127, validation loss: 7398.079\n",
      "Epoch:  151, Step:  1359, training loss: 7398.263, validation loss: 7397.594\n",
      "Epoch:  152, Step:  1368, training loss: 7395.953, validation loss: 7397.510\n",
      "Epoch:  153, Step:  1377, training loss: 7399.009, validation loss: 7397.952\n",
      "Epoch:  154, Step:  1386, training loss: 7399.363, validation loss: 7398.297\n",
      "Epoch:  155, Step:  1395, training loss: 7396.553, validation loss: 7397.658\n",
      "Epoch:  156, Step:  1404, training loss: 7396.427, validation loss: 7397.500\n",
      "Epoch:  157, Step:  1413, training loss: 7393.997, validation loss: 7397.332\n",
      "Epoch:  158, Step:  1422, training loss: 7398.361, validation loss: 7397.305\n",
      "Epoch:  159, Step:  1431, training loss: 7397.002, validation loss: 7397.562\n",
      "Epoch:  160, Step:  1440, training loss: 7394.713, validation loss: 7397.273\n",
      "Epoch:  161, Step:  1449, training loss: 7398.981, validation loss: 7397.325\n",
      "Learning rate decreased to 0.00540360\n",
      "Epoch:  162, Step:  1458, training loss: 7395.625, validation loss: 7397.972\n",
      "Epoch:  163, Step:  1467, training loss: 7398.777, validation loss: 7397.366\n",
      "Epoch:  164, Step:  1476, training loss: 7398.757, validation loss: 7397.771\n",
      "Epoch:  165, Step:  1485, training loss: 7398.863, validation loss: 7397.759\n",
      "Epoch:  166, Step:  1494, training loss: 7396.947, validation loss: 7397.688\n",
      "Epoch:  167, Step:  1503, training loss: 7396.854, validation loss: 7397.245\n",
      "Epoch:  168, Step:  1512, training loss: 7399.890, validation loss: 7398.084\n",
      "Learning rate decreased to 0.00513342\n",
      "Epoch:  169, Step:  1521, training loss: 7396.644, validation loss: 7398.094\n",
      "Epoch:  170, Step:  1530, training loss: 7397.925, validation loss: 7397.662\n",
      "Epoch:  171, Step:  1539, training loss: 7399.104, validation loss: 7397.249\n",
      "Epoch:  172, Step:  1548, training loss: 7396.943, validation loss: 7397.712\n",
      "Epoch:  173, Step:  1557, training loss: 7396.897, validation loss: 7397.710\n",
      "Epoch:  174, Step:  1566, training loss: 7394.307, validation loss: 7397.454\n",
      "Epoch:  175, Step:  1575, training loss: 7400.799, validation loss: 7397.533\n",
      "Learning rate decreased to 0.00487675\n",
      "Epoch:  176, Step:  1584, training loss: 7396.874, validation loss: 7397.906\n",
      "Epoch:  177, Step:  1593, training loss: 7397.039, validation loss: 7397.312\n",
      "Epoch:  178, Step:  1602, training loss: 7396.575, validation loss: 7397.240\n",
      "Epoch:  179, Step:  1611, training loss: 7397.357, validation loss: 7397.187\n",
      "Epoch:  180, Step:  1620, training loss: 7395.712, validation loss: 7397.593\n",
      "Epoch:  181, Step:  1629, training loss: 7399.285, validation loss: 7397.217\n",
      "Epoch:  182, Step:  1638, training loss: 7399.187, validation loss: 7397.901\n",
      "Epoch:  183, Step:  1647, training loss: 7399.538, validation loss: 7398.108\n",
      "Learning rate decreased to 0.00463291\n",
      "Epoch:  184, Step:  1656, training loss: 7396.537, validation loss: 7397.592\n",
      "Epoch:  185, Step:  1665, training loss: 7399.921, validation loss: 7397.573\n",
      "Epoch:  186, Step:  1674, training loss: 7399.197, validation loss: 7397.881\n",
      "Epoch:  187, Step:  1683, training loss: 7399.722, validation loss: 7397.814\n",
      "Epoch:  188, Step:  1692, training loss: 7399.841, validation loss: 7397.543\n",
      "Epoch:  189, Step:  1701, training loss: 7398.082, validation loss: 7397.567\n",
      "Epoch:  190, Step:  1710, training loss: 7397.863, validation loss: 7397.249\n",
      "Epoch:  191, Step:  1719, training loss: 7398.516, validation loss: 7397.300\n",
      "Epoch:  192, Step:  1728, training loss: 7397.510, validation loss: 7397.847\n",
      "Epoch:  193, Step:  1737, training loss: 7395.429, validation loss: 7397.240\n",
      "Epoch:  194, Step:  1746, training loss: 7394.371, validation loss: 7397.221\n",
      "Epoch:  195, Step:  1755, training loss: 7396.483, validation loss: 7397.205\n",
      "Epoch:  196, Step:  1764, training loss: 7396.592, validation loss: 7397.279\n",
      "Epoch:  197, Step:  1773, training loss: 7394.847, validation loss: 7397.674\n",
      "Epoch:  198, Step:  1782, training loss: 7399.935, validation loss: 7397.258\n",
      "Learning rate decreased to 0.00440127\n",
      "Epoch:  199, Step:  1791, training loss: 7399.909, validation loss: 7397.407\n",
      "Epoch:  200, Step:  1800, training loss: 7396.626, validation loss: 7397.899\n",
      "Epoch:  201, Step:  1809, training loss: 7398.852, validation loss: 7397.830\n",
      "Epoch:  202, Step:  1818, training loss: 7400.007, validation loss: 7397.437\n",
      "Epoch:  203, Step:  1827, training loss: 7396.894, validation loss: 7397.539\n",
      "Epoch:  204, Step:  1836, training loss: 7399.785, validation loss: 7397.192\n",
      "Epoch:  205, Step:  1845, training loss: 7396.195, validation loss: 7397.389\n",
      "Epoch:  206, Step:  1854, training loss: 7394.503, validation loss: 7397.150\n",
      "Epoch:  207, Step:  1863, training loss: 7396.767, validation loss: 7397.275\n",
      "Epoch:  208, Step:  1872, training loss: 7396.256, validation loss: 7397.744\n",
      "Epoch:  209, Step:  1881, training loss: 7396.822, validation loss: 7397.275\n",
      "Epoch:  210, Step:  1890, training loss: 7399.301, validation loss: 7397.536\n",
      "Epoch:  211, Step:  1899, training loss: 7397.729, validation loss: 7397.720\n",
      "Epoch:  212, Step:  1908, training loss: 7396.166, validation loss: 7397.188\n",
      "Epoch:  213, Step:  1917, training loss: 7399.576, validation loss: 7397.159\n",
      "Learning rate decreased to 0.00418120\n",
      "Epoch:  214, Step:  1926, training loss: 7396.250, validation loss: 7397.413\n",
      "Epoch:  215, Step:  1935, training loss: 7398.327, validation loss: 7398.037\n",
      "Epoch:  216, Step:  1944, training loss: 7397.159, validation loss: 7397.280\n",
      "Epoch:  217, Step:  1953, training loss: 7394.673, validation loss: 7397.206\n",
      "Epoch:  218, Step:  1962, training loss: 7400.548, validation loss: 7397.461\n",
      "Epoch:  219, Step:  1971, training loss: 7399.821, validation loss: 7397.733\n",
      "Epoch:  220, Step:  1980, training loss: 7399.193, validation loss: 7397.778\n",
      "Epoch:  221, Step:  1989, training loss: 7397.866, validation loss: 7397.554\n",
      "Epoch:  222, Step:  1998, training loss: 7396.500, validation loss: 7397.368\n",
      "Epoch:  223, Step:  2007, training loss: 7399.783, validation loss: 7397.119\n",
      "Epoch:  224, Step:  2016, training loss: 7398.118, validation loss: 7397.696\n",
      "Epoch:  225, Step:  2025, training loss: 7400.123, validation loss: 7397.917\n",
      "Learning rate decreased to 0.00397214\n",
      "Epoch:  226, Step:  2034, training loss: 7398.789, validation loss: 7397.682\n",
      "Epoch:  227, Step:  2043, training loss: 7398.944, validation loss: 7397.299\n",
      "Epoch:  228, Step:  2052, training loss: 7400.140, validation loss: 7397.549\n",
      "Epoch:  229, Step:  2061, training loss: 7399.507, validation loss: 7397.514\n",
      "Epoch:  230, Step:  2070, training loss: 7396.219, validation loss: 7397.569\n",
      "Epoch:  231, Step:  2079, training loss: 7398.994, validation loss: 7397.122\n",
      "Epoch:  232, Step:  2088, training loss: 7396.887, validation loss: 7397.459\n",
      "Epoch:  233, Step:  2097, training loss: 7397.743, validation loss: 7397.690\n",
      "Epoch:  234, Step:  2106, training loss: 7396.461, validation loss: 7397.173\n",
      "Epoch:  235, Step:  2115, training loss: 7397.764, validation loss: 7397.114\n",
      "Epoch:  236, Step:  2124, training loss: 7396.585, validation loss: 7397.574\n",
      "Epoch:  237, Step:  2133, training loss: 7397.416, validation loss: 7397.892\n",
      "Epoch:  238, Step:  2142, training loss: 7399.841, validation loss: 7397.820\n",
      "Learning rate decreased to 0.00377354\n",
      "Epoch:  239, Step:  2151, training loss: 7399.048, validation loss: 7397.790\n",
      "Epoch:  240, Step:  2160, training loss: 7397.893, validation loss: 7397.311\n",
      "Epoch:  241, Step:  2169, training loss: 7395.866, validation loss: 7397.151\n",
      "Epoch:  242, Step:  2178, training loss: 7400.151, validation loss: 7397.092\n",
      "Epoch:  243, Step:  2187, training loss: 7396.105, validation loss: 7397.456\n",
      "Epoch:  244, Step:  2196, training loss: 7396.209, validation loss: 7397.917\n",
      "Epoch:  245, Step:  2205, training loss: 7397.134, validation loss: 7397.464\n",
      "Epoch:  246, Step:  2214, training loss: 7400.624, validation loss: 7397.135\n",
      "Learning rate decreased to 0.00358486\n",
      "Epoch:  247, Step:  2223, training loss: 7399.095, validation loss: 7397.613\n",
      "Epoch:  248, Step:  2232, training loss: 7397.694, validation loss: 7397.772\n",
      "Epoch:  249, Step:  2241, training loss: 7400.186, validation loss: 7397.155\n",
      "Epoch:  250, Step:  2250, training loss: 7400.222, validation loss: 7397.471\n",
      "Epoch:  251, Step:  2259, training loss: 7396.469, validation loss: 7397.744\n",
      "Epoch:  252, Step:  2268, training loss: 7397.991, validation loss: 7397.572\n",
      "Epoch:  253, Step:  2277, training loss: 7395.975, validation loss: 7397.110\n",
      "Epoch:  254, Step:  2286, training loss: 7399.731, validation loss: 7397.108\n",
      "Epoch:  255, Step:  2295, training loss: 7398.942, validation loss: 7397.457\n",
      "Epoch:  256, Step:  2304, training loss: 7395.454, validation loss: 7397.819\n",
      "Epoch:  257, Step:  2313, training loss: 7396.147, validation loss: 7397.117\n",
      "Epoch:  258, Step:  2322, training loss: 7399.781, validation loss: 7397.312\n",
      "Learning rate decreased to 0.00340562\n",
      "Epoch:  259, Step:  2331, training loss: 7396.969, validation loss: 7397.512\n",
      "Epoch:  260, Step:  2340, training loss: 7394.683, validation loss: 7397.005\n",
      "Epoch:  261, Step:  2349, training loss: 7398.321, validation loss: 7397.228\n",
      "Epoch:  262, Step:  2358, training loss: 7400.509, validation loss: 7397.142\n",
      "Epoch:  263, Step:  2367, training loss: 7396.174, validation loss: 7397.636\n",
      "Epoch:  264, Step:  2376, training loss: 7397.022, validation loss: 7397.180\n",
      "Epoch:  265, Step:  2385, training loss: 7399.510, validation loss: 7397.348\n",
      "Epoch:  266, Step:  2394, training loss: 7399.438, validation loss: 7397.581\n",
      "Epoch:  267, Step:  2403, training loss: 7399.225, validation loss: 7397.859\n",
      "Epoch:  268, Step:  2412, training loss: 7399.574, validation loss: 7397.598\n",
      "Epoch:  269, Step:  2421, training loss: 7395.282, validation loss: 7397.365\n",
      "Epoch:  270, Step:  2430, training loss: 7396.799, validation loss: 7397.047\n",
      "Epoch:  271, Step:  2439, training loss: 7400.021, validation loss: 7397.051\n",
      "Learning rate decreased to 0.00323534\n",
      "Epoch:  272, Step:  2448, training loss: 7395.525, validation loss: 7397.371\n",
      "Epoch:  273, Step:  2457, training loss: 7399.404, validation loss: 7397.166\n",
      "Epoch:  274, Step:  2466, training loss: 7396.101, validation loss: 7397.329\n",
      "Epoch:  275, Step:  2475, training loss: 7398.734, validation loss: 7397.653\n",
      "Epoch:  276, Step:  2484, training loss: 7396.707, validation loss: 7397.690\n",
      "Epoch:  277, Step:  2493, training loss: 7397.616, validation loss: 7397.485\n",
      "Epoch:  278, Step:  2502, training loss: 7397.539, validation loss: 7397.482\n",
      "Epoch:  279, Step:  2511, training loss: 7395.233, validation loss: 7397.406\n",
      "Epoch:  280, Step:  2520, training loss: 7397.866, validation loss: 7397.001\n",
      "Epoch:  281, Step:  2529, training loss: 7399.771, validation loss: 7397.032\n",
      "Learning rate decreased to 0.00307357\n",
      "Epoch:  282, Step:  2538, training loss: 7397.062, validation loss: 7397.484\n",
      "Epoch:  283, Step:  2547, training loss: 7396.430, validation loss: 7397.117\n",
      "Epoch:  284, Step:  2556, training loss: 7398.058, validation loss: 7397.389\n",
      "Epoch:  285, Step:  2565, training loss: 7395.951, validation loss: 7397.635\n",
      "Epoch:  286, Step:  2574, training loss: 7399.749, validation loss: 7397.608\n",
      "Epoch:  287, Step:  2583, training loss: 7398.579, validation loss: 7397.415\n",
      "Epoch:  288, Step:  2592, training loss: 7399.918, validation loss: 7397.272\n",
      "Learning rate decreased to 0.00291989\n",
      "Epoch:  289, Step:  2601, training loss: 7396.521, validation loss: 7397.281\n",
      "Epoch:  290, Step:  2610, training loss: 7395.817, validation loss: 7397.499\n",
      "Epoch:  291, Step:  2619, training loss: 7397.120, validation loss: 7397.512\n",
      "Epoch:  292, Step:  2628, training loss: 7398.278, validation loss: 7397.118\n",
      "Epoch:  293, Step:  2637, training loss: 7400.134, validation loss: 7397.213\n",
      "Epoch:  294, Step:  2646, training loss: 7399.321, validation loss: 7397.721\n",
      "Epoch:  295, Step:  2655, training loss: 7399.827, validation loss: 7397.686\n",
      "Epoch:  296, Step:  2664, training loss: 7399.555, validation loss: 7397.530\n",
      "Epoch:  297, Step:  2673, training loss: 7399.608, validation loss: 7397.560\n",
      "Epoch:  298, Step:  2682, training loss: 7396.798, validation loss: 7397.526\n",
      "Epoch:  299, Step:  2691, training loss: 7396.463, validation loss: 7397.430\n",
      "Epoch:  300, Step:  2700, training loss: 7395.695, validation loss: 7397.432\n",
      "Epoch:  301, Step:  2709, training loss: 7397.552, validation loss: 7397.400\n",
      "Epoch:  302, Step:  2718, training loss: 7399.629, validation loss: 7397.444\n",
      "Learning rate decreased to 0.00277390\n",
      "Epoch:  303, Step:  2727, training loss: 7395.493, validation loss: 7397.253\n",
      "Epoch:  304, Step:  2736, training loss: 7397.936, validation loss: 7396.960\n",
      "Epoch:  305, Step:  2745, training loss: 7396.181, validation loss: 7397.202\n",
      "Epoch:  306, Step:  2754, training loss: 7396.908, validation loss: 7397.557\n",
      "Epoch:  307, Step:  2763, training loss: 7396.233, validation loss: 7397.087\n",
      "Epoch:  308, Step:  2772, training loss: 7399.089, validation loss: 7396.974\n",
      "Epoch:  309, Step:  2781, training loss: 7398.558, validation loss: 7397.240\n",
      "Epoch:  310, Step:  2790, training loss: 7396.018, validation loss: 7397.717\n",
      "Epoch:  311, Step:  2799, training loss: 7396.180, validation loss: 7397.705\n",
      "Epoch:  312, Step:  2808, training loss: 7399.821, validation loss: 7397.540\n",
      "Learning rate decreased to 0.00263520\n",
      "Epoch:  313, Step:  2817, training loss: 7395.953, validation loss: 7397.390\n",
      "Epoch:  314, Step:  2826, training loss: 7400.056, validation loss: 7397.366\n",
      "Epoch:  315, Step:  2835, training loss: 7398.188, validation loss: 7397.489\n",
      "Epoch:  316, Step:  2844, training loss: 7399.367, validation loss: 7397.494\n",
      "Epoch:  317, Step:  2853, training loss: 7399.240, validation loss: 7397.403\n",
      "Epoch:  318, Step:  2862, training loss: 7398.629, validation loss: 7397.296\n",
      "Epoch:  319, Step:  2871, training loss: 7397.108, validation loss: 7397.198\n",
      "Epoch:  320, Step:  2880, training loss: 7397.713, validation loss: 7396.993\n",
      "Epoch:  321, Step:  2889, training loss: 7396.426, validation loss: 7397.000\n",
      "Epoch:  322, Step:  2898, training loss: 7399.411, validation loss: 7397.318\n",
      "Learning rate decreased to 0.00250344\n",
      "Epoch:  323, Step:  2907, training loss: 7399.196, validation loss: 7397.604\n",
      "Epoch:  324, Step:  2916, training loss: 7395.367, validation loss: 7397.598\n",
      "Epoch:  325, Step:  2925, training loss: 7394.539, validation loss: 7396.995\n",
      "Epoch:  326, Step:  2934, training loss: 7400.479, validation loss: 7397.012\n",
      "Epoch:  327, Step:  2943, training loss: 7398.278, validation loss: 7397.050\n",
      "Epoch:  328, Step:  2952, training loss: 7397.028, validation loss: 7397.289\n",
      "Epoch:  329, Step:  2961, training loss: 7396.403, validation loss: 7397.610\n",
      "Epoch:  330, Step:  2970, training loss: 7398.627, validation loss: 7397.574\n",
      "Epoch:  331, Step:  2979, training loss: 7399.234, validation loss: 7397.379\n",
      "Epoch:  332, Step:  2988, training loss: 7399.444, validation loss: 7397.526\n",
      "Epoch:  333, Step:  2997, training loss: 7399.100, validation loss: 7397.467\n",
      "Epoch:  334, Step:  3006, training loss: 7398.958, validation loss: 7397.415\n",
      "Epoch:  335, Step:  3015, training loss: 7395.937, validation loss: 7397.347\n",
      "Epoch:  336, Step:  3024, training loss: 7399.521, validation loss: 7397.304\n",
      "Learning rate decreased to 0.00237827\n",
      "Epoch:  337, Step:  3033, training loss: 7396.706, validation loss: 7397.264\n",
      "Epoch:  338, Step:  3042, training loss: 7399.088, validation loss: 7396.995\n",
      "Epoch:  339, Step:  3051, training loss: 7395.968, validation loss: 7397.063\n",
      "Epoch:  340, Step:  3060, training loss: 7399.030, validation loss: 7396.983\n",
      "Epoch:  341, Step:  3069, training loss: 7399.241, validation loss: 7397.301\n",
      "Epoch:  342, Step:  3078, training loss: 7398.338, validation loss: 7397.587\n",
      "Epoch:  343, Step:  3087, training loss: 7395.466, validation loss: 7397.487\n",
      "Epoch:  344, Step:  3096, training loss: 7396.067, validation loss: 7396.922\n",
      "Epoch:  345, Step:  3105, training loss: 7399.589, validation loss: 7396.934\n",
      "Learning rate decreased to 0.00225936\n",
      "Epoch:  346, Step:  3114, training loss: 7396.288, validation loss: 7397.113\n",
      "Epoch:  347, Step:  3123, training loss: 7394.663, validation loss: 7396.941\n",
      "Epoch:  348, Step:  3132, training loss: 7399.569, validation loss: 7396.878\n",
      "Epoch:  349, Step:  3141, training loss: 7394.982, validation loss: 7397.065\n",
      "Epoch:  350, Step:  3150, training loss: 7394.593, validation loss: 7396.891\n",
      "Epoch:  351, Step:  3159, training loss: 7396.589, validation loss: 7396.854\n",
      "Epoch:  352, Step:  3168, training loss: 7396.741, validation loss: 7396.930\n",
      "Epoch:  353, Step:  3177, training loss: 7399.642, validation loss: 7397.196\n",
      "Learning rate decreased to 0.00214639\n",
      "Epoch:  354, Step:  3186, training loss: 7395.713, validation loss: 7397.229\n",
      "Epoch:  355, Step:  3195, training loss: 7398.657, validation loss: 7396.985\n",
      "Epoch:  356, Step:  3204, training loss: 7398.138, validation loss: 7397.009\n",
      "Epoch:  357, Step:  3213, training loss: 7396.323, validation loss: 7397.208\n",
      "Epoch:  358, Step:  3222, training loss: 7396.809, validation loss: 7397.331\n",
      "Epoch:  359, Step:  3231, training loss: 7395.071, validation loss: 7397.035\n",
      "Epoch:  360, Step:  3240, training loss: 7398.812, validation loss: 7396.943\n",
      "Learning rate decreased to 0.00203907\n",
      "Epoch:  361, Step:  3249, training loss: 7399.421, validation loss: 7397.035\n",
      "Epoch:  362, Step:  3258, training loss: 7396.327, validation loss: 7397.239\n",
      "Epoch:  363, Step:  3267, training loss: 7396.409, validation loss: 7397.023\n",
      "Epoch:  364, Step:  3276, training loss: 7395.588, validation loss: 7397.246\n",
      "Epoch:  365, Step:  3285, training loss: 7396.226, validation loss: 7397.384\n",
      "Epoch:  366, Step:  3294, training loss: 7397.577, validation loss: 7397.321\n",
      "Epoch:  367, Step:  3303, training loss: 7396.581, validation loss: 7397.056\n",
      "Epoch:  368, Step:  3312, training loss: 7395.769, validation loss: 7397.163\n",
      "Epoch:  369, Step:  3321, training loss: 7398.439, validation loss: 7397.255\n",
      "Learning rate decreased to 0.00193711\n",
      "Epoch:  370, Step:  3330, training loss: 7399.859, validation loss: 7397.189\n",
      "Epoch:  371, Step:  3339, training loss: 7399.852, validation loss: 7397.278\n",
      "Epoch:  372, Step:  3348, training loss: 7396.485, validation loss: 7397.451\n",
      "Epoch:  373, Step:  3357, training loss: 7398.122, validation loss: 7397.302\n",
      "Epoch:  374, Step:  3366, training loss: 7395.572, validation loss: 7397.050\n",
      "Epoch:  375, Step:  3375, training loss: 7395.561, validation loss: 7396.879\n",
      "Epoch:  376, Step:  3384, training loss: 7400.084, validation loss: 7396.910\n",
      "Learning rate decreased to 0.00184026\n",
      "Epoch:  377, Step:  3393, training loss: 7397.872, validation loss: 7396.952\n",
      "Epoch:  378, Step:  3402, training loss: 7398.353, validation loss: 7397.102\n",
      "Epoch:  379, Step:  3411, training loss: 7399.847, validation loss: 7397.192\n",
      "Epoch:  380, Step:  3420, training loss: 7396.171, validation loss: 7397.251\n",
      "Epoch:  381, Step:  3429, training loss: 7396.079, validation loss: 7396.933\n",
      "Epoch:  382, Step:  3438, training loss: 7395.812, validation loss: 7396.978\n",
      "Epoch:  383, Step:  3447, training loss: 7397.168, validation loss: 7396.867\n",
      "Epoch:  384, Step:  3456, training loss: 7396.044, validation loss: 7397.013\n",
      "Epoch:  385, Step:  3465, training loss: 7399.708, validation loss: 7396.992\n",
      "Epoch:  386, Step:  3474, training loss: 7396.327, validation loss: 7397.155\n",
      "Epoch:  387, Step:  3483, training loss: 7395.677, validation loss: 7397.319\n",
      "Epoch:  388, Step:  3492, training loss: 7395.701, validation loss: 7397.369\n",
      "Epoch:  389, Step:  3501, training loss: 7397.261, validation loss: 7397.375\n",
      "Epoch:  390, Step:  3510, training loss: 7399.797, validation loss: 7397.040\n",
      "Learning rate decreased to 0.00174825\n",
      "Epoch:  391, Step:  3519, training loss: 7399.389, validation loss: 7397.097\n",
      "Epoch:  392, Step:  3528, training loss: 7398.304, validation loss: 7397.329\n",
      "Epoch:  393, Step:  3537, training loss: 7398.695, validation loss: 7397.444\n",
      "Epoch:  394, Step:  3546, training loss: 7396.393, validation loss: 7397.398\n",
      "Epoch:  395, Step:  3555, training loss: 7396.492, validation loss: 7397.298\n",
      "Epoch:  396, Step:  3564, training loss: 7397.224, validation loss: 7396.940\n",
      "Epoch:  397, Step:  3573, training loss: 7395.967, validation loss: 7396.864\n",
      "Epoch:  398, Step:  3582, training loss: 7399.813, validation loss: 7396.860\n",
      "Learning rate decreased to 0.00166083\n",
      "Epoch:  399, Step:  3591, training loss: 7398.232, validation loss: 7397.038\n",
      "Epoch:  400, Step:  3600, training loss: 7399.687, validation loss: 7397.453\n",
      "Epoch:  401, Step:  3609, training loss: 7396.823, validation loss: 7397.487\n",
      "Epoch:  402, Step:  3618, training loss: 7398.852, validation loss: 7397.458\n",
      "Epoch:  403, Step:  3627, training loss: 7394.892, validation loss: 7397.333\n",
      "Epoch:  404, Step:  3636, training loss: 7398.658, validation loss: 7396.991\n",
      "Epoch:  405, Step:  3645, training loss: 7398.633, validation loss: 7396.943\n",
      "Epoch:  406, Step:  3654, training loss: 7396.331, validation loss: 7397.083\n",
      "Epoch:  407, Step:  3663, training loss: 7398.246, validation loss: 7397.245\n",
      "Epoch:  408, Step:  3672, training loss: 7396.361, validation loss: 7397.312\n",
      "Epoch:  409, Step:  3681, training loss: 7395.002, validation loss: 7397.231\n",
      "Epoch:  410, Step:  3690, training loss: 7398.724, validation loss: 7396.905\n",
      "Learning rate decreased to 0.00157779\n",
      "Epoch:  411, Step:  3699, training loss: 7394.294, validation loss: 7396.925\n",
      "Epoch:  412, Step:  3708, training loss: 7398.675, validation loss: 7396.911\n",
      "Epoch:  413, Step:  3717, training loss: 7396.152, validation loss: 7396.927\n",
      "Epoch:  414, Step:  3726, training loss: 7398.380, validation loss: 7396.908\n",
      "Epoch:  415, Step:  3735, training loss: 7398.627, validation loss: 7397.135\n",
      "Epoch:  416, Step:  3744, training loss: 7394.799, validation loss: 7397.320\n",
      "Epoch:  417, Step:  3753, training loss: 7397.648, validation loss: 7397.057\n",
      "Epoch:  418, Step:  3762, training loss: 7395.638, validation loss: 7397.069\n",
      "Epoch:  419, Step:  3771, training loss: 7394.791, validation loss: 7397.128\n",
      "Epoch:  420, Step:  3780, training loss: 7399.379, validation loss: 7396.939\n",
      "Learning rate decreased to 0.00149890\n",
      "Epoch:  421, Step:  3789, training loss: 7399.666, validation loss: 7397.017\n",
      "Epoch:  422, Step:  3798, training loss: 7399.479, validation loss: 7397.199\n",
      "Epoch:  423, Step:  3807, training loss: 7399.494, validation loss: 7397.384\n",
      "Epoch:  424, Step:  3816, training loss: 7397.897, validation loss: 7397.490\n",
      "Epoch:  425, Step:  3825, training loss: 7397.253, validation loss: 7397.299\n",
      "Epoch:  426, Step:  3834, training loss: 7397.684, validation loss: 7396.894\n",
      "Epoch:  427, Step:  3843, training loss: 7398.736, validation loss: 7396.831\n",
      "Epoch:  428, Step:  3852, training loss: 7396.399, validation loss: 7396.944\n",
      "Epoch:  429, Step:  3861, training loss: 7397.062, validation loss: 7396.815\n",
      "Epoch:  430, Step:  3870, training loss: 7396.034, validation loss: 7396.981\n",
      "Epoch:  431, Step:  3879, training loss: 7394.568, validation loss: 7397.168\n",
      "Epoch:  432, Step:  3888, training loss: 7395.485, validation loss: 7396.924\n",
      "Epoch:  433, Step:  3897, training loss: 7394.199, validation loss: 7396.841\n",
      "Epoch:  434, Step:  3906, training loss: 7400.276, validation loss: 7396.838\n",
      "Learning rate decreased to 0.00142396\n",
      "Epoch:  435, Step:  3915, training loss: 7398.260, validation loss: 7396.882\n",
      "Epoch:  436, Step:  3924, training loss: 7399.226, validation loss: 7397.116\n",
      "Epoch:  437, Step:  3933, training loss: 7396.293, validation loss: 7397.340\n",
      "Epoch:  438, Step:  3942, training loss: 7396.966, validation loss: 7397.078\n",
      "Epoch:  439, Step:  3951, training loss: 7399.738, validation loss: 7396.958\n",
      "Epoch:  440, Step:  3960, training loss: 7395.974, validation loss: 7397.023\n",
      "Epoch:  441, Step:  3969, training loss: 7399.301, validation loss: 7397.168\n",
      "Epoch:  442, Step:  3978, training loss: 7395.997, validation loss: 7397.229\n",
      "Epoch:  443, Step:  3987, training loss: 7397.465, validation loss: 7396.966\n",
      "Epoch:  444, Step:  3996, training loss: 7396.551, validation loss: 7396.846\n",
      "Epoch:  445, Step:  4005, training loss: 7394.577, validation loss: 7396.747\n",
      "Epoch:  446, Step:  4014, training loss: 7399.872, validation loss: 7396.745\n",
      "Learning rate decreased to 0.00135276\n",
      "Epoch:  447, Step:  4023, training loss: 7398.915, validation loss: 7396.879\n",
      "Epoch:  448, Step:  4032, training loss: 7395.954, validation loss: 7397.086\n",
      "Epoch:  449, Step:  4041, training loss: 7397.726, validation loss: 7396.936\n",
      "Epoch:  450, Step:  4050, training loss: 7399.365, validation loss: 7396.913\n",
      "Epoch:  451, Step:  4059, training loss: 7397.668, validation loss: 7396.999\n",
      "Epoch:  452, Step:  4068, training loss: 7394.245, validation loss: 7396.968\n",
      "Epoch:  453, Step:  4077, training loss: 7399.511, validation loss: 7396.860\n",
      "Learning rate decreased to 0.00128512\n",
      "Epoch:  454, Step:  4086, training loss: 7396.752, validation loss: 7396.941\n",
      "Epoch:  455, Step:  4095, training loss: 7396.206, validation loss: 7397.091\n",
      "Epoch:  456, Step:  4104, training loss: 7396.750, validation loss: 7397.214\n",
      "Epoch:  457, Step:  4113, training loss: 7398.735, validation loss: 7397.056\n",
      "Epoch:  458, Step:  4122, training loss: 7399.465, validation loss: 7397.099\n",
      "Epoch:  459, Step:  4131, training loss: 7396.004, validation loss: 7397.229\n",
      "Epoch:  460, Step:  4140, training loss: 7397.819, validation loss: 7397.265\n",
      "Epoch:  461, Step:  4149, training loss: 7398.046, validation loss: 7397.135\n",
      "Epoch:  462, Step:  4158, training loss: 7395.745, validation loss: 7397.113\n",
      "Epoch:  463, Step:  4167, training loss: 7399.262, validation loss: 7396.920\n",
      "Epoch:  464, Step:  4176, training loss: 7397.277, validation loss: 7396.946\n",
      "Epoch:  465, Step:  4185, training loss: 7396.182, validation loss: 7397.104\n",
      "Epoch:  466, Step:  4194, training loss: 7397.732, validation loss: 7397.217\n",
      "Epoch:  467, Step:  4203, training loss: 7399.405, validation loss: 7397.140\n",
      "Learning rate decreased to 0.00122087\n",
      "Epoch:  468, Step:  4212, training loss: 7399.104, validation loss: 7397.166\n",
      "Epoch:  469, Step:  4221, training loss: 7398.400, validation loss: 7397.300\n",
      "Epoch:  470, Step:  4230, training loss: 7399.231, validation loss: 7397.383\n",
      "Epoch:  471, Step:  4239, training loss: 7395.022, validation loss: 7397.261\n",
      "Epoch:  472, Step:  4248, training loss: 7397.510, validation loss: 7396.951\n",
      "Epoch:  473, Step:  4257, training loss: 7399.894, validation loss: 7396.858\n",
      "Learning rate decreased to 0.00115982\n",
      "Epoch:  474, Step:  4266, training loss: 7394.432, validation loss: 7396.908\n",
      "Epoch:  475, Step:  4275, training loss: 7394.462, validation loss: 7396.838\n",
      "Epoch:  476, Step:  4284, training loss: 7394.390, validation loss: 7396.811\n",
      "Epoch:  477, Step:  4293, training loss: 7396.840, validation loss: 7396.833\n",
      "Epoch:  478, Step:  4302, training loss: 7396.502, validation loss: 7396.855\n",
      "Epoch:  479, Step:  4311, training loss: 7397.378, validation loss: 7396.866\n",
      "Epoch:  480, Step:  4320, training loss: 7396.330, validation loss: 7397.011\n",
      "Epoch:  481, Step:  4329, training loss: 7397.960, validation loss: 7397.244\n",
      "Learning rate decreased to 0.00110183\n",
      "Epoch:  482, Step:  4338, training loss: 7394.838, validation loss: 7397.288\n",
      "Epoch:  483, Step:  4347, training loss: 7396.604, validation loss: 7397.029\n",
      "Epoch:  484, Step:  4356, training loss: 7394.757, validation loss: 7396.993\n",
      "Epoch:  485, Step:  4365, training loss: 7397.736, validation loss: 7396.859\n",
      "Epoch:  486, Step:  4374, training loss: 7394.425, validation loss: 7396.812\n",
      "Epoch:  487, Step:  4383, training loss: 7396.356, validation loss: 7396.787\n",
      "Epoch:  488, Step:  4392, training loss: 7398.595, validation loss: 7396.811\n",
      "Learning rate decreased to 0.00104674\n",
      "Epoch:  489, Step:  4401, training loss: 7399.393, validation loss: 7396.858\n",
      "Epoch:  490, Step:  4410, training loss: 7398.699, validation loss: 7397.042\n",
      "Epoch:  491, Step:  4419, training loss: 7396.307, validation loss: 7397.106\n",
      "Epoch:  492, Step:  4428, training loss: 7397.033, validation loss: 7396.969\n",
      "Epoch:  493, Step:  4437, training loss: 7398.501, validation loss: 7396.907\n",
      "Epoch:  494, Step:  4446, training loss: 7396.790, validation loss: 7396.950\n",
      "Epoch:  495, Step:  4455, training loss: 7398.226, validation loss: 7396.944\n",
      "Epoch:  496, Step:  4464, training loss: 7399.188, validation loss: 7397.025\n",
      "Learning rate decreased to 0.00099440\n",
      "Epoch:  497, Step:  4473, training loss: 7399.421, validation loss: 7397.126\n",
      "Epoch:  498, Step:  4482, training loss: 7394.768, validation loss: 7397.140\n",
      "Epoch:  499, Step:  4491, training loss: 7395.734, validation loss: 7396.955\n",
      "Epoch:  500, Step:  4500, training loss: 7396.588, validation loss: 7396.958\n",
      "...training complete.\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, valid_ds, max_epochs=1000, batch_size=100, use_tensorboard=False,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "158e48f0-608a-4593-8ae4-af3cbdcf4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = np.mean(np.asarray(model.infer_factors(Test_data)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d92d0c6-3473-444e-98bf-c22760993078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([401, 100, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbf8b728-b63b-4e45-962e-58777b9e2562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16320696100002116"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## quick checking the leanred smoothness of embeddings \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "lfads_embedding =preprocessing.MinMaxScaler().fit_transform(\n",
    "                                pca.fit_transform(np.mean(np.asarray(factors), axis = 1)))\n",
    "\n",
    "\n",
    "def compute_smoothness(data):\n",
    "    diff = np.diff(data, axis=0)\n",
    "    bending_energy = np.sum(np.linalg.norm(diff, axis=1)**2)  ## conventional 1-degree time difference\n",
    "    return 1/bending_energy   ### to ensure it is monotonic increasing properity \n",
    "\n",
    "compute_smoothness(lfads_embedding[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4812e4f1-6714-4c4e-b0c4-5b592f53528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Quick visualization to check its overview trend\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(3,3), dpi = 100)\n",
    "ax = fig.add_subplot(111)\n",
    "time_points = np.arange(lfads_embedding.shape[0])\n",
    "sc = ax.scatter(lfads_embedding[:,0],\n",
    "                lfads_embedding[:,1],\n",
    "           c=time_points,\n",
    "           cmap='coolwarm', s=40)\n",
    "\n",
    "ax.grid(False)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9140c4ca-7100-4466-8f8c-3ca8e3ece17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'lfads_demo/'\n",
      "/home/cbi/wjbai/lfads-torch/lfads_demo\n"
     ]
    }
   ],
   "source": [
    "%cd lfads_demo/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b050d42-3c0e-433f-a254-61b82ea31e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(401, 100, 100)\n",
      "(401, 100, 100)\n",
      "=== Run 01/10 ===\n",
      "Random seed: 6753\n",
      "Beginning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbi/wjbai/lfads-torch/lfads_demo/utils.py:91: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return yaml.load(open(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1, Step:     5, training loss: 9981.841, validation loss: 10910.515\n",
      "Epoch:    2, Step:    10, training loss: 9322.038, validation loss: 10483.086\n",
      "Epoch:    3, Step:    15, training loss: 9042.218, validation loss: 10000.094\n",
      "Epoch:    4, Step:    20, training loss: 8764.722, validation loss: 9193.683\n",
      "Epoch:    5, Step:    25, training loss: 8497.478, validation loss: 8619.742\n",
      "Epoch:    6, Step:    30, training loss: 8238.989, validation loss: 8263.377\n",
      "Epoch:    7, Step:    35, training loss: 8017.285, validation loss: 8019.731\n",
      "Epoch:    8, Step:    40, training loss: 7841.438, validation loss: 7829.849\n",
      "Epoch:    9, Step:    45, training loss: 7694.259, validation loss: 7682.039\n",
      "Epoch:   10, Step:    50, training loss: 7581.180, validation loss: 7583.406\n",
      "Epoch:   11, Step:    55, training loss: 7508.220, validation loss: 7538.751\n",
      "Epoch:   12, Step:    60, training loss: 7455.792, validation loss: 7486.416\n",
      "Epoch:   13, Step:    65, training loss: 7431.730, validation loss: 7448.136\n",
      "Epoch:   14, Step:    70, training loss: 7414.811, validation loss: 7436.006\n",
      "Epoch:   15, Step:    75, training loss: 7406.792, validation loss: 7438.119\n",
      "Epoch:   16, Step:    80, training loss: 7414.710, validation loss: 7473.303\n",
      "Epoch:   17, Step:    85, training loss: 7411.306, validation loss: 7425.741\n",
      "Epoch:   18, Step:    90, training loss: 7400.561, validation loss: 7420.339\n",
      "Epoch:   19, Step:    95, training loss: 7400.318, validation loss: 7423.719\n",
      "Epoch:   20, Step:   100, training loss: 7399.697, validation loss: 7409.280\n",
      "Epoch:   21, Step:   105, training loss: 7394.592, validation loss: 7410.392\n"
     ]
    }
   ],
   "source": [
    "%run run_sim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7a06ffc-a98d-4255-b251-ca1e98580bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 401, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-stitched mode\n",
    "embeddings = np.load('sim_lfads_embeddings.npy')\n",
    "\n",
    "embeddings.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139aa40e-eabc-49fd-871f-a56d284863c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
